vagrant command

vagrant init data-science-toolbox/data-science-at-the-command-line

turn on : vagrant up
connect : vagrant ssh
disconnect : exit
turn off : vagrant halt
delete : vagrant destroy

**shell command

head -n 3 data/movies.txt

echo 'Hello'\
> ' world' |
> wc

echo 'Hello world' | wc


fac() { (echo 1; seq $1) | paste -s -d\* | bc; }
fac 5
120

alias l='ls -1 --group-directories-first'
alias moer=more

type -a pwd
type -a cd
type -a fac
type -a l

seq 30 | grep 3
3
13
23
30

seq 100 | grep 3 | wc -l
19

seq 10 > data/ten-numbers

echo -n "Hello" > hello-world
echo " World" >> hello-world

cat hello-world | wc -w
< hello-world wc -w
wc -w hello-world

mv hello-world data
mv hello-world old-file
rm old-file
rm -r ~/book/ch02/data/old
cp server.log server.log.bak

man cat | head -n 20
help cd | head -n 20
jq --help

tar -xzvf data/logs.tar.gz

in2csv data/imdb-250.xlsx > data/imdb-250.csv
in2csv data/imdb-250.xlsx | head | cut -c1-80
in2csv data/imdb-250.xlsx | head | csvcut -c Title,Year,Rating | csvlook

sql2csv --db 'sqlite:///data/iris.db' --query 'SELECT * FROM iris WHERE sepal_length > 7.5'
sql2csv --db 'sqlite:///data/iris.db' --query 'SELECT * FROM iris WHERE sepal_length > 7.5' | csvlook

curl -s http://www.gutenberg.org/ebooks/epub/76/pg76.txt | head -n 10
 : -s for silent means diable downlaod status (currently the url not possible)
curl http://www.gutenberg.org/cache/epub/76/pg76.txt > data/finn.txt
curl -s http://www.gutenberg.org/cache/epub/76/pg76.txt -o data/finn.txt
curl -u username:password ftp://host/file

curl -L j.mp/locatbbar(-L or --location)
curl j.mp/locatbbar
curl -I j.mp/locatbbar

curl -s http://api.randomuser.me | jq '.'

=============================================================================
chapter 4 make shell script

curl -s http://www.gutenberg.org/cache/epub/76/pg76.txt | tr '[:upper:]' '[:lower:]' | grep -oE '\w+' | sort | uniq -c | sort -nr | head -n 10

1. Copy and paste the one-liner into a file.
2. Add execute permissions.
3. Define a so-called shebang.
4. Remove the fixed input part.
5. Add a parameter.
6. Optionally extend your PATH.

step 1 : make file
bash ~/book/ch04/top-words-1.sh
echo "!!" > scriptname
!!

step 2 : permission
chmod u+x top-words-2.sh
ls -l top-words-{1,2}.sh
-rw-rw-r-- 1 vagrant vagrant 145 Jul 20 23:33 top-words-1.sh
-rwxrw-r-- 1 vagrant vagrant 143 Jul 20 23:34 top-words-2.sh

r:read w:wrote x:excutable
first 1 char : -:regular file d:directory
next 3 char : owner persmossions
next 3 char : group owner permissions
last 3 char : all other users permissions

./top-words-2.sh

step 3 : Define Shebang
#!/usr/bin/env bash

step 4 : Remove Fixed Input
#!/usr/bin/env bash
tr '[:upper:]' '[:lower:]' | grep -oE '\w+' | sort |
uniq -c | sort -nr | head -n 10

step 5 : Parameterize
#!/usr/bin/env bash
NUM_WORDS="$1"
tr '[:upper:]' '[:lower:]' | grep -oE '\w+' | sort |
uniq -c | sort -nr | head -n $NUM_WORDS

cat data/finn.txt | ./top-words-5.sh 5

step 6 : extend PATH vatiable
echo $PATH | fold
echo $PATH | tr : '\n' | sort
**edit the .bashrc or .profle file

** Pthon and R
#!/usr/bin/env python
from sys import stdin, stdout
while True:
line = stdin.readline()
if not line:
break
stdout.write("%d\n" % int(line)**2)
stdout.flush()

#!/usr/bin/env Rscript
f <- file("stdin")
open(f)
while(length(line <- readLines(f, n = 1)) > 0) {
write(as.integer(line)^2, stdout())
}
close(f)

========================================================================
Chapter 5 Scrubbing Data

echo 'foo\nbar\nfoo' | sort | uniq -c | sort -nr
2 foo
1 bar
--> ?? not possible. need to fix

echo 'foo\nbar\nfoo' | sort | uniq -c | sort -nr |
> awk '{print $2","$1}' | header -a value,count

value,count
foo,2
bar,1
--> ?? not possible. need to fix

cd ~/book/ch05/data
seq -f "Line %g" 10 | tee lines
Line 1
Line 2
Line 3
Line 4
Line 5
Line 6
Line 7
Line 8
Line 9
Line 10

< lines head -n 3
< lines sed -n '1,3p'
< lines awk 'NR<=3'
Line 1
Line 2
Line 3

< lines tail -n 3
Line 8
Line 9
Line 10

< lines tail -n +4
< lines sed '1,3d'
< lines sed -n '1,3!p'
Line 4
Line 5
Line 6
Line 7
Line 8
Line 9
Line 10

< lines head -n -3
Line 1
Line 2
Line 3
Line 4
Line 5
Line 6
Line 7

< lines sed -n '4,6p'
< lines awk '(NR>=4)&&(NR<=6)'
< lines head -n 6 | tail -n 3
Line 4
Line 5
Line 6

< lines sed -n '1~2p'
< lines awk 'NR%2'
Line 1
Line 3
Line 5
Line 7
Line 9

< lines sed -n '0~2p'
< lines awk '(NR+1)%2'
Line 2
Line 4
Line 6
Line 8
Line 10

**based on pattern 
grep -i chapter alice.txt
(-i means case-insensitive)
CHAPTER I. Down the Rabbit-Hole
CHAPTER II. The Pool of Tears
CHAPTER III. A Caucus-Race and a Long Tale
CHAPTER IV. The Rabbit Sends in a Little Bill
CHAPTER V. Advice from a Caterpillar
CHAPTER VI. Pig and Pepper
CHAPTER VII. A Mad Tea-Party
CHAPTER VIII. The Queen's Croquet-Ground
CHAPTER IX. The Mock Turtle's Story
CHAPTER X. The Lobster Quadrille
CHAPTER XI. Who Stole the Tarts?
CHAPTER XII. Alice's Evidence

grep -E '^CHAPTER (.*)\. The' alice.txt
-E option in order to enable regular expressions
CHAPTER II. The Pool of Tears
CHAPTER IV. The Rabbit Sends in a Little Bill
CHAPTER VIII. The Queen's Croquet-Ground
CHAPTER IX. The Mock Turtle's Story
CHAPTER X. The Lobster Quadrille

**based on randomness 
seq 1000 | sample -r 1% | jq -c '{line: .}'
{"line":69}
{"line":239}
{"line":278}
{"line":457}
{"line":480}
{"line":670}
{"line":746}
{"line":827}
{"line":850}
{"line":854}
{"line":855}
{"line":998}

seq 10000 | sample -r 1% -d 1000 -s 5 | jq -c '{line: .}'
{"line":45}
{"line":78}
{"line":299}
{"line":406}
{"line":472}
{"line":648}


** Extracting Values

grep -i chapter alice.txt | cut -d' ' -f3-
Down the Rabbit-Hole
The Pool of Tears
A Caucus-Race and a Long Tale
The Rabbit Sends in a Little Bill
Advice from a Caterpillar
Pig and Pepper
A Mad Tea-Party
The Queen's Croquet-Ground
The Mock Turtle's Story
The Lobster Quadrille
Who Stole the Tarts?
Alice's Evidence

sed -rn 's/^CHAPTER ([IVXLCDM]{1,})\. (.*)$/\2/p' alice.txt > /dev/null


grep -i chapter alice.txt | cut -c 9-
I. Down the Rabbit-Hole
II. The Pool of Tears
III. A Caucus-Race and a Long Tale
IV. The Rabbit Sends in a Little Bill
V. Advice from a Caterpillar
VI. Pig and Pepper
VII. A Mad Tea-Party
VIII. The Queen's Croquet-Ground
IX. The Mock Turtle's Story
X. The Lobster Quadrille
XI. Who Stole the Tarts?
XII. Alice's Evidence

sed -rn 's/^CHAPTER ([IVXLCDM]{1,})\. (.*)$/\2/p' alice.txt > /dev/nul

< alice.txt grep -oE '\w{2,}' | head
Project
Gutenberg
Alice
Adventures
in
Wonderland
by
Lewis
Carroll
This

< alice.txt tr '[:upper:]' '[:lower:]' | grep -oE '\w{2,}' | grep -E '^a.*e$' | sort | uniq -c | sort -nr | awk '{print $2","$1}' | header -a word,count | head | csvlook
|-------------+--------|
|  word       | count  |
|-------------+--------|
|  alice      | 403    |
|  are        | 73     |
|  archive    | 13     |
|  agree      | 11     |
|  anyone     | 5      |
|  alone      | 5      |
|  age        | 4      |
|  applicable | 3      |
|  anywhere   | 3      |
|-------------+--------|

echo 'hello world!' | tr ' ' '_'
hello_world!

echo 'hello world!' | tr ' !' '_?'
hello_world?

echo 'hello world!' | tr -d -c '[a-z]'
helloworld

echo 'hello world!' | tr '[a-z]' '[A-Z]'
echo 'hello world!' | tr '[:lower:]' '[:upper:]'
HELLO WORLD!

echo ' hello world!' | sed -re 's/hello/bye/;s/\s+/ /g;s/\s+//'
bye world!

echo -e "value\n7\n2\n5\n3" | body sort -n
seq 5 | header -a count
count
1 
2 
3 
4 
5

seq 5 | header -a count | wc -l
6

seq 5 | header -a count | body wc -l
count
5

< tips.csv header
< tips.csv head -n 1

seq 5 | header -a count
echo "count" | cat - <(seq 5)
count
1 
2 
3 
4 
5

< iris.csv header -d | head
seq 5 | header -a line | body wc -l | header -r count
count
5

seq 5 | header -a line | header -e "tr '[a-z]' '[A-Z]'"
LINE
1 
2 
3 
4 
5

< tips.csv cols head | csvlook
|--------+------+--------+--------+-----+--------+-------|
|  bill  | tip  | sex    | smoker | day | time   | size  |
|--------+------+--------+--------+-----+--------+-------|
|  16.99 | 1.01 | Female | No     | Sun | Dinner | 2     |
|  10.34 | 1.66 | Male   | No     | Sun | Dinner | 3     |
|  21.01 | 3.5  | Male   | No     | Sun | Dinner | 3     |
|  23.68 | 3.31 | Male   | No     | Sun | Dinner | 2     |
|  24.59 | 3.61 | Female | No     | Sun | Dinner | 4     |
|  25.29 | 4.71 | Male   | No     | Sun | Dinner | 4     |
|  8.77  | 2.0  | Male   | No     | Sun | Dinner | 2     |
|  26.88 | 3.12 | Male   | No     | Sun | Dinner | 4     |
|  15.04 | 1.96 | Male   | No     | Sun | Dinner | 2     |
|--------+------+--------+--------+-----+--------+-------|

< tips.csv cols -c day | head | csvlook
|---+-------+------+--------+--------+--------+-------|
|   | bill  | tip  | sex    | smoker | time   | size  |
|---+-------+------+--------+--------+--------+-------|
|   | 16.99 | 1.01 | Female | No     | Dinner | 2     |
|   | 10.34 | 1.66 | Male   | No     | Dinner | 3     |
|   | 21.01 | 3.5  | Male   | No     | Dinner | 3     |
|   | 23.68 | 3.31 | Male   | No     | Dinner | 2     |
|   | 24.59 | 3.61 | Female | No     | Dinner | 4     |
|   | 25.29 | 4.71 | Male   | No     | Dinner | 4     |
|   | 8.77  | 2.0  | Male   | No     | Dinner | 2     |
|   | 26.88 | 3.12 | Male   | No     | Dinner | 4     |
|   | 15.04 | 1.96 | Male   | No     | Dinner | 2     |
|---+-------+------+--------+--------+--------+-------|

< tips.csv cols -c day body "tr '[a-z]' '[A-Z]'" | head | csvlook
|------+-------+------+--------+--------+--------+-------|
|  DAY | bill  | tip  | sex    | smoker | time   | size  |
|------+-------+------+--------+--------+--------+-------|
|  SUN | 16.99 | 1.01 | Female | No     | Dinner | 2     |
|  SUN | 10.34 | 1.66 | Male   | No     | Dinner | 3     |
|  SUN | 21.01 | 3.5  | Male   | No     | Dinner | 3     |
|  SUN | 23.68 | 3.31 | Male   | No     | Dinner | 2     |
|  SUN | 24.59 | 3.61 | Female | No     | Dinner | 4     |
|  SUN | 25.29 | 4.71 | Male   | No     | Dinner | 4     |
|  SUN | 8.77  | 2.0  | Male   | No     | Dinner | 2     |
|  SUN | 26.88 | 3.12 | Male   | No     | Dinner | 4     |
|  SUN | 15.04 | 1.96 | Male   | No     | Dinner | 2     |
|------+-------+------+--------+--------+--------+-------|

Performing SQL Queries on CSV


seq 5 | header -a value | csvsql --query "SELECT SUM(value) AS sum FROM stdin"
sum
15

sed -e 's/"gender":/"sex":/g' data/users.json | fold | head -n 3

**********Working with HTML/XML and JSON******************
curl -sL 'http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_border/area_ratio' > wiki.html

head -n 10 data/wiki.html | cut -c1-79
< wiki.html grep wikitable -A 21
< wiki.html scrape -b -e 'table.wikitable > tr:not(:first-child)' > table.html
head -n 21 data/table.html

< table.html xml2json > table.json
< table.json jq '.' | head -n 25

< table.json jq -c '.html.body.tr[] | {country: .td[1][],border:.td[2][], surface: .td[3][]}' > countries.json
head -n 10 data/countries.json
head -n 11 countries.csv | csvlook
*************************

curl -sL 'http://en.wikipedia.org/wiki/List_of_countries'\
> '_and_territories_by_border/area_ratio' |
> scrape -be 'table.wikitable > tr:not(:first-child)' |
> xml2json | jq -c '.html.body.tr[] | {country: .td[1][],'\
> 'border: .td[2][], surface: .td[3][], ratio: .td[4][]}' |
> json2csv -p -k=border,surface | head -n 11 | csvlook

****Common Scrub Operations for CSV
Extracting and Reordering Columns
< iris.csv csvcut -c sepal_length,petal_length,sepal_width,petal_width |
head -n 5 | csvlook
< iris.csv csvcut -C species | head -n 5 | csvlook

echo -e 'a,b,c,d,e,f,g,h,i\n1,2,3,4,5,6,7,8,9' | csvcut -c $(seq 1 2 9 | paste -sd,)
a,c,e,g,i
1,3,5,7,9

echo -e 'a,b,c,d,e,f,g,h,i\n1,2,3,4,5,6,7,8,9' | cut -d, -f 5,1,3
a,c,e
1,3,5


< iris.csv csvsql --query "SELECT sepal_length, petal_length, sepal_width, petal_width FROM stdin" | head -n 5 | csvlook

******Filtering Lines*******
seq 5 | sed -n '3,5p'
seq 5 | header -a count | body sed -n '3,5p'

csvgrep:
to exclude all the bills of which the party size was 4 or less:
csvgrep -c size -i -r "[1-4]" tips.csv | csvlook
|--------+------+--------+--------+------+--------+-------|
|  bill  | tip  | sex    | smoker | day  | time   | size  |
|--------+------+--------+--------+------+--------+-------|
|  29.8  | 4.2  | Female | No     | Thur | Lunch  | 6     |
|  34.3  | 6.7  | Male   | No     | Thur | Lunch  | 6     |
|  41.19 | 5.0  | Male   | No     | Thur | Lunch  | 5     |
|  27.05 | 5.0  | Female | No     | Thur | Lunch  | 6     |
|  29.85 | 5.14 | Female | No     | Sun  | Dinner | 5     |
|  48.17 | 5.0  | Male   | No     | Sun  | Dinner | 6     |
|  20.69 | 5.0  | Male   | No     | Sun  | Dinner | 5     |
|  30.46 | 2.0  | Male   | Yes    | Sun  | Dinner | 5     |
|  28.15 | 3.0  | Male   | Yes    | Sat  | Dinner | 5     |
|--------+------+--------+--------+------+--------+-------|

awk:
to get all the bills above 40 USD on a Saturday or a Sunday:
< tips.csv awk -F, '($1 > 40.0) && ($5 ~ /S/)' | csvlook
|--------+------+--------+-----+-----+--------+----|
|  48.27 | 6.73 | Male   | No  | Sat | Dinner | 4  |
|--------+------+--------+-----+-----+--------+----|
|  44.3  | 2.5  | Female | Yes | Sat | Dinner | 3  |
|  48.17 | 5.0  | Male   | No  | Sun | Dinner | 6  |
|  50.81 | 10.0 | Male   | Yes | Sat | Dinner | 3  |
|  45.35 | 3.5  | Male   | Yes | Sun | Dinner | 3  |
|  40.55 | 3.0  | Male   | Yes | Sun | Dinner | 2  |
|  48.33 | 9.0  | Male   | No  | Sat | Dinner | 4  |
|--------+------+--------+-----+-----+--------+----|

csvsql:
< tips.csv csvsql --query "SELECT * FROM stdin WHERE bill > 40 AND day LIKE '%S%'" | csvlook

|--------+------+--------+--------+-----+--------+-------|
|  bill  | tip  | sex    | smoker | day | time   | size  |
|--------+------+--------+--------+-----+--------+-------|
|  48.27 | 6.73 | Male   | 0      | Sat | Dinner | 4     |
|  44.3  | 2.5  | Female | 1      | Sat | Dinner | 3     |
|  48.17 | 5.0  | Male   | 0      | Sun | Dinner | 6     |
|  50.81 | 10.0 | Male   | 1      | Sat | Dinner | 3     |
|  45.35 | 3.5  | Male   | 1      | Sun | Dinner | 3     |
|  40.55 | 3.0  | Male   | 1      | Sun | Dinner | 2     |
|  48.33 | 9.0  | Male   | 0      | Sat | Dinner | 4     |
|--------+------+--------+--------+-----+--------+-------|

************Merging Columns*****************

< names.csv csvlook
|-----+-----------+------------+-------|
|  id | last_name | first_name | born  |
|-----+-----------+------------+-------|
|  1  | Williams  | John       | 1932  |
|  2  | Elfman    | Danny      | 1953  |
|  3  | Horner    | James      | 1953  |
|  4  | Shore     | Howard     | 1946  |
|  5  | Zimmer    | Hans       | 1957  |
|-----+-----------+------------+-------|

sed : 
< names.csv sed -re '1s/.*/id,full_name,born/g;2,$s/(.*),(.*),(.*),(.*)/\1,\3 \2,\4/g' | csvlook
|-----+---------------+-------|
|  id | full_name     | born  |
|-----+---------------+-------|
|  1  | John Williams | 1932  |
|  2  | Danny Elfman  | 1953  |
|  3  | James Horner  | 1953  |
|  4  | Howard Shore  | 1946  |
|  5  | Hans Zimmer   | 1957  |
|-----+---------------+-------|

awk :
< names.csv awk -F, 'BEGIN{OFS=","; print "id,full_name,born"}{if(NR > 1) {print $1,$3" "$2,$4}}' | csvlook
|-----+---------------+-------|
|  id | full_name     | born  |
|-----+---------------+-------|
|  1  | John Williams | 1932  |
|  2  | Danny Elfman  | 1953  |
|  3  | James Horner  | 1953  |
|  4  | Howard Shore  | 1946  |
|  5  | Hans Zimmer   | 1957  |
|-----+---------------+-------|

cols : 
< names.csv cols -c first_name,last_name tr \",\" \" \" | header -r full_name,id,born | csvcut -c id,full_name,born | csvlook
|-----+---------------+-------|
|  id | full_name     | born  |
|-----+---------------+-------|
|  1  | John Williams | 1932  |
|  2  | Danny Elfman  | 1953  |
|  3  | James Horner  | 1953  |
|  4  | Howard Shore  | 1946  |
|  5  | Hans Zimmer   | 1957  |
|-----+---------------+-------|

csvsql : 
< names.csv csvsql --query "SELECT id, first_name || ' ' || last_name AS full_name, born FROM stdin" | csvlook
|-----+---------------+-------|
|  id | full_name     | born  |
|-----+---------------+-------|
|  1  | John Williams | 1932  |
|  2  | Danny Elfman  | 1953  |
|  3  | James Horner  | 1953  |
|  4  | Howard Shore  | 1946  |
|  5  | Hans Zimmer   | 1957  |
|-----+---------------+-------|

What if last_name contained a comma?
sed:
< names-comma.csv sed -re '1s/.*/id,full_name,born/g;2,$s/(.*),(.*),(.*),(.*)/\1,\3 \2,\4/g' | tail -n 1
6,"Beethoven,Ludwig  van",1770 (fail)

awk:
< names-comma.csv awk -F, 'BEGIN{OFS=","; print "id,full_name,born"} {if(NR > 1) {print $1,$3" "$2,$4}}' | tail -n 1
6, van" "Beethoven,Ludwig (fail)

cols:
< names-comma.csv cols -c first_name,last_name tr \",\" \" \" | header -r full_name,id,born | csvcut -c id,full_name,born | tail -n 1
6,"Ludwig ""Beethoven  van""",1770 (fail)

csvsql :
< names-comma.csv csvsql --query "SELECT id, first_name || ' ' || last_name AS full_name, born FROM stdin" | tail -n 1
6,"Ludwig Beethoven, van",1770 (succed)

Rio(R script) :
< names-comma.csv Rio -e 'df$full_name <- paste(df$first_name,df$last_name); df[c("id","full_name","born")]' | tail -n 1
6,"Ludwig Beethoven, van",1770 (succed)

************Combining Multiple CSV Files*****************
Concatenate vertically
< iris.csv fieldsplit -d, -k -F species -p . -s .csv
wc -l Iris-*.csv

cat:
cat Iris-setosa.csv <(< Iris-versicolor.csv header -d) <(< Iris-virginica.csv header -d) | sed -n '1p;49,54p' | csvlook
|---------------+-------------+--------------+-------------+------------------|
|  sepal_length | sepal_width | petal_length | petal_width | species          |
|---------------+-------------+--------------+-------------+------------------|
|  4.6          | 3.2         | 1.4          | 0.2         | Iris-setosa      |
|  5.3          | 3.7         | 1.5          | 0.2         | Iris-setosa      |
|  5.0          | 3.3         | 1.4          | 0.2         | Iris-setosa      |
|  7.0          | 3.2         | 4.7          | 1.4         | Iris-versicolor  |
|  6.4          | 3.2         | 4.5          | 1.5         | Iris-versicolor  |
|  6.9          | 3.1         | 4.9          | 1.5         | Iris-versicolor  |
|---------------+-------------+--------------+-------------+------------------|

csvstack Iris-*.csv | sed -n '1p;49,54p' | csvlook
|---------------+-------------+--------------+-------------+------------------|
|  sepal_length | sepal_width | petal_length | petal_width | species          |
|---------------+-------------+--------------+-------------+------------------|
|  4.6          | 3.2         | 1.4          | 0.2         | Iris-setosa      |
|  5.3          | 3.7         | 1.5          | 0.2         | Iris-setosa      |
|  5.0          | 3.3         | 1.4          | 0.2         | Iris-setosa      |
|  7.0          | 3.2         | 4.7          | 1.4         | Iris-versicolor  |
|  6.4          | 3.2         | 4.5          | 1.5         | Iris-versicolor  |
|  6.9          | 3.1         | 4.9          | 1.5         | Iris-versicolor  |
|---------------+-------------+--------------+-------------+------------------|

mark filename
csvstack Iris-*.csv -n species --filenames

add group name
csvstack Iris-*.csv -n class -g a,b,c | csvcut -C species |sed -n '1p;49,54p' | csvlook
|--------+--------------+-------------+--------------+--------------|
|  class | sepal_length | sepal_width | petal_length | petal_width  |
|--------+--------------+-------------+--------------+--------------|
|  a     | 4.6          | 3.2         | 1.4          | 0.2          |
|  a     | 5.3          | 3.7         | 1.5          | 0.2          |
|  a     | 5.0          | 3.3         | 1.4          | 0.2          |
|  b     | 7.0          | 3.2         | 4.7          | 1.4          |
|  b     | 6.4          | 3.2         | 4.5          | 1.5          |
|  b     | 6.9          | 3.1         | 4.9          | 1.5          |
|--------+--------------+-------------+--------------+--------------|

Concatenate horizontally
< tips.csv csvcut -c bill,tip | tee bills.csv | head -n 5 | csvlook
|--------+-------|
|  bill  | tip   |
|--------+-------|
|  16.99 | 1.01  |
|  10.34 | 1.66  |
|  21.01 | 3.5   |
|  23.68 | 3.31  |
|--------+-------|

< tips.csv csvcut -c day,time | tee datetime.csv | head -n 3 | csvlook
|------+---------|
|  day | time    |
|------+---------|
|  Sun | Dinner  |
|  Sun | Dinner  |
|------+---------|

< tips.csv csvcut -c sex,smoker,size | tee customers.csv |head -n 3 | csvlook
|---------+--------+-------|
|  sex    | smoker | size  |
|---------+--------+-------|
|  Female | No     | 2     |
|  Male   | No     | 3     |
|---------+--------+-------|

paste -d, {bills,customers,datetime}.csv | head -n 3 | csvlook
|--------+------+--------+--------+------+-----+---------|
|  bill  | tip  | sex    | smoker | size | day | time    |
|--------+------+--------+--------+------+-----+---------|
|  16.99 | 1.01 | Female | No     | 2    | Sun | Dinner  |
|  10.34 | 1.66 | Male   | No     | 3    | Sun | Dinner  |
|--------+------+--------+--------+------+-----+---------|

Joining
csvjoin -c species iris.csv irismeta.csv | csvcut -c sepal_length,sepal_width,species,usda_id | sed -n '1p;49,54p' | csvlook
|---------------+-------------+-----------------+----------|
|  sepal_length | sepal_width | species         | usda_id  |
|---------------+-------------+-----------------+----------|
|  4.6          | 3.2         | Iris-setosa     | IRSE     |
|  5.3          | 3.7         | Iris-setosa     | IRSE     |
|  5.0          | 3.3         | Iris-setosa     | IRSE     |
|  7.0          | 3.2         | Iris-versicolor | IRVE2    |
|  6.4          | 3.2         | Iris-versicolor | IRVE2    |
|  6.9          | 3.1         | Iris-versicolor | IRVE2    |
|---------------+-------------+-----------------+----------|

csvsql:
csvsql --query 'SELECT i.sepal_length, i.sepal_width, i.species, m.usda_id FROM iris i JOIN irismeta m ON (i.species = m.species)'  iris.csv irismeta.csv | sed -n '1p;49,54p' | csvlook
|---------------+-------------+-----------------+----------|
|  sepal_length | sepal_width | species         | usda_id  |
|---------------+-------------+-----------------+----------|
|  4.6          | 3.2         | Iris-setosa     | IRSE     |
|  5.3          | 3.7         | Iris-setosa     | IRSE     |
|  5.0          | 3.3         | Iris-setosa     | IRSE     |
|  7.0          | 3.2         | Iris-versicolor | IRVE2    |
|  6.4          | 3.2         | Iris-versicolor | IRVE2    |
|  6.9          | 3.1         | Iris-versicolor | IRVE2    |
|---------------+-------------+-----------------+----------|


=====================================================================
Chapter 06 : Drake

curl -s 'http://www.gutenberg.org/browse/scores/top' |
grep -E '^<li>' |
head -n 5 |
sed -E "s/.*ebooks\/([0-9]+).*/\\1/" > data/top-5

=====================================================================
Chapter 07 : Exploring Data

head file.csv | csvlook

less -S file.csv

< data/iris.csv sed -e 's/,/\n/g;q'

csvsql data/datatypes.csv

cat data/iris.csv | csvcut -c species | body "sort | uniq | wc -l"

csvstat data/investments2.csv --unique

csvstat data/datatypes.csv
  --null
  --max
  --min
  --sum
  --mean
  --median
  --stdev
  --nulls
  --unique
  --freq
  --len
csvstat data/datatypes.csv --null
csvstat data/investments2.csv -c 2,13,19,24
csvstat data/iris.csv | tail -n 1
csvstat data/iris.csv | sed -rne '${s/^([^:]+): ([0-9]+)$/\2/;p}'

***** Rio ****************
< data/tips.csv Rio -e 'df$tip / df$bill * 100' | head
< data/tips.csv Rio -e 'df$percent <- df$tip / df$bill * 100; df' | head
< data/iris.csv Rio -e 'mean(df$sepal_length)'
< data/iris.csv Rio -e 'sd(df$sepal_length)'
< data/iris.csv Rio -e 'sum(df$sepal_length)'

< data/iris.csv Rio -e 'summary(df$sepal_length)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  4.300   5.100   5.800   5.843   6.400   7.900

< data/iris.csv Rio -e 'skewness(df$sepal_length)'
< data/iris.csv Rio -e 'kurtosis(df$petal_width)'
< dat/iris.csv Rio -e 'cor(df$bill, df$tip)'
< data/tips.csv csvcut -c bill,tip | Rio -f cor | csvlook

< data/iris.csv Rio -e 'stem(df$sepal_length)'

 The decimal point is 1 digit(s) to the left of the |

  42 | 0
  44 | 0000
  46 | 000000
  48 | 00000000000
  50 | 0000000000000000000
  52 | 00000
  54 | 0000000000000
  56 | 00000000000000
  58 | 0000000000
  60 | 000000000000
  62 | 0000000000000
  64 | 000000000000
  66 | 0000000000
  68 | 0000000
  70 | 00
  72 | 0000
  74 | 0
  76 | 00000
  78 | 0

**** visualization *******
while true; do echo $RANDOM; done | sample -d 10 | feedgnuplot --stream \
--terminal 'dumb 80,25' --lines --xlen 10

  30000 ++-----+------------+-------------+-------------+------------+-----++
        |      +            +      **     +             +            +      |
        |      :            :    **  *    :             :            :      |
  25000 ++.....************....**.....**...................................++
        |      *           *:**         * :             :            :      |
        |     *:            *            *:             :            :      |
        |     *:            :             :             :            :      |
  20000 ++...*............................*................................++
        |    * :            :             :*            :            :      |
        |   *  :            :             :*            :            :      |
  15000 ++..*...............................*...........**.................++
        |  *   :            :             :  *         *: *          :      |
        |  *   :            :             :  *        * :  **        :      |
  10000 ++*...................................*......*.......*.............++
        | *    :            :             :   *      *  :            :      |
        |*     :            :             :    *    *   :     *******:   ****
        |*     :            :             :     *  *    :            ****   |
   5000 *+......................................*.*........................++
        |      :            :             :      *      :            :      |
        |      +            +             +             +            +      |
      0 ++-----+------------+-------------+-------------+------------+-----++
              2280         2282          2284          2286         2288

**** ggplot2 package with Rio ******

< data/immigration.dat sed -re '/^#/d;s/\t/,/g;s/,-,/,0,/g;s/Region/'\
'Period/' | tee data/immigration.csv | head | cut -c1-80


< data/immigration.csv csvcut -c Period,Denmark,netherlands,Norway,Sweden | Rio -re 'melt(df, id="Period", variable.name="Country", vaue.name="Count")' | tee data/immigration-long.csv


< data/immigration-long.csv Rio -ge 'g + geom_bar(aes(Country, Count,fill=Period), stat="identity") + scale_fill_brewer(palette="Set1")+ labs(x="Country of origin", y="Immigration by decade", title="Immigration from Northern Europe\n(columstacked histogram)")' > image.png

**Histograms
**Rio
< data/tips.csv Rio -ge 'g+geom_histogram(aes(bill))' > /vagrant/image2.png

**feedgnuplot
< data/tips.csv csvcut -c bill | feedgnuplot --terminal 'dumb 80,25' \
--histogram 0 --with boxes --ymin 0 --binwidth 1.5 --unset grid --exit

  25 ++----+------+-----+--***-+-----+------+-----+------+-----+------+----++
     +     +      +     +*** * +     +      +     +      +     +      +     +
     |                   * * *                                              |
     |               *** * * *                                              |
  20 ++              * * * * *                                             ++
     |            **** * * * *                                              |
     |            * ** *** * * ***                                          |
     |            * ** * * * * * *                                          |
  15 ++           * ** * * * * * *                                         ++
     |            * ** * * * * * *                                          |
     |            * ** * * * * * *                                          |
     |            * ** * * * * * * ***                                      |
  10 ++           * ** * * * *** *** *                                     ++
     |            * ** * * * * * * * *                                      |
     |          *** ** * * * * * * * ***** ***                              |
     |          * * ** * * * * * * * * * *** *                              |
   5 ++       *** * ** * * * * * * * * * * * *   ***                       ++
     |        * * * ** * * * * * * * * * * * * *** *                        |
     |        * * * ** * * * * * * * * * * * *** * ********   *** ***       |
     +  ***+*** * * ** *+* * * * * * * * * *+* * *+** * *+* ***+* * * ***   +
   0 ++-***+***********************************************-*****-***-***--++
     0     5      10    15     20    25     30    35     40    45     50    55

** Bar Plots
** Rio
< data/tips.csv Rio -ge 'g+geom_bar(aes(factor(size)))' > /vagrant/image3.png

**feedgnuplot
< data/tips.csv csvcut -c size | header -d | feedgnuplot --terminal 'dumb 80,25' --histogram 0 --with boxes --unset grid --exit

** Density plot
** Rio

< data/tips.csv Rio -ge 'g+geom_density(aes(tip / bill * 100, fill=sex), alpha=0.3) + xlab("percent")' > /vagrant/image4.png

** Box Plots
** Rio
< data/tips.csv Rio -ge 'g+geom_boxplot(aes(time, bill))' > /vagrant/image5_box.png

**Scatter Plots
**Rio
< data/tips.csv Rio -ge 'g+geom_point(aes(bill, tip, color=time))' > /vagrant/image6-scatter.png

**feedgnuplot
< data/tips.csv csvcut -c bill,tip | tr , ' ' | header -d | feedgnuplot --terminal 'dumb 80,25' --points --domain --unset grid --exit --style pt 14
  10 ++----+------+-----+------+-----+------+-----+------+-----+------+A---++
     +     +      +     +      +     +      +     +      +     +      +     +
   9 ++                                                            A       ++
     |                                                                      |
   8 ++                                                                    ++
     |                                                  A                   |
     |                                                                      |
   7 ++                                          A                 A       ++
     |                             A     A                                  |
   6 ++                              A    A    A                           ++
     |                             A        A                               |
   5 ++       A                 A A   A A   AA A  AA      A  A     A       ++
     |                                A       A    A     A                  |
   4 ++          A     A  AAAA AAA A  A A  A          A                    ++
     |                A   AAAAA AAA AA            A             A           |
     |              A  AAAAAAA AA A A  AA   A AA                            |
   3 ++           A   AAAAAAAAAAA A A    AA           AA A                 ++
     |              AAAAAAA AA  A A A     A                   A             |
   2 ++        AA AAAAAAAAA A  A  A AA  A A A                              ++
     +     +   AAAAAAAA +A   AA+     + A    +     +      +     +      +     +
   1 ++--A-+A-A---+--AA-+--A---+-----+------+--A--+------+-----+------+----++
     0     5      10    15     20    25     30    35     40    45     50    55

** Line Graphs
** Rio
< data/immigration-long.csv Rio -ge 'g+geom_line(aes(x=Period, y=Count, group=Country, color=Country)) + theme(axis.text.x =element_text(angle = -45, hjust = 0))' > /vagrant/image7-line.png

** feedgnuplot
< data/immigration.csv csvcut -c Period,Denmark,Netherlands,Norway,Sweden | header -d | tr , ' ' | feedgnuplot --terminal 'dumb 80,25' --lines --autolegend --domain --legend 0 "Denmark" --legend 1 "Netherlands"  --legend 2 "Norway" --legend 3 "Sweden" --xlabel "Period" --unset grid --exit
  250000 ++-----%%%-------+-------+--------+-------+-------+--------+------++
         +  %%%% + %      +       +        +       +       + Denmark+****** +
         |%%        %                                    Netherlands ###### |
         |          %                                         Norway $$$$$$ |
  200000 ++          %                                        Sweden %%%%%%++
         |        $   %                                                     |
         |       $ $   %                                                    |
         |      $   $  %                                                    |
  150000 ++   $$     $  %                                                  ++
         |   $        $  %                                                  |
         |  $          $  %                                                 |
  100000 ++$            $ %                                                ++
         |$              $ %%%%%%%%%%                                       |
         |                $          %                                      |
         |    ***********  $$$$$$$$$$$%                                     |
   50000 +****  #########**           $%%                  #######         ++
         |  ####           ********    $$%              ###       ##        |
         |##                       ******##           ##$$$$$$$$$$$$#       |
         +       +        +       +      **###########$$*************       +
       0 ++------+--------+-------+--------*************---+--------+------++
        1890    1900     1910    1920     1930    1940    1950     1960    1970
                                       Period


=============================================================
Chapter 8 Parallel Pipelines

** bc : calculation tool
echo "4^2" | bc

** for - do - done
for i in {0..100..2}
>do
>echo "$i^2" | bc
>done | tail

< data/users.json jq -r '.results[].user.email' > data/emails.txt
cat data/emails.txt
kaylee.anderson64@example.com
arthur.baker92@example.com
chloe.graham66@example.com
wyatt.nelson80@example.com
peter.coleman75@example.com

while read line
> do
> echo "Sending invitation to ${line}."
> done < data/emails.txt
Sending invitation to kaylee.anderson64@example.com.
Sending invitation to arthur.baker92@example.com.
Sending invitation to chloe.graham66@example.com.
Sending invitation to wyatt.nelson80@example.com.
Sending invitation to peter.coleman75@example.com.

while read i; do echo "input:$i.";done < /dev/stdin
one
input:one.
two
input:two.
three
input:three.


for filename in *.csv
> do
> echo "Processing ${filename}."
> done
Processing *.csv.


find data -name '*.json' -exec echo "Processing {}" \;
Processing data/_users.json
Processing data/users.json

** Parallel Processing
#!/bin/bash
echo "Starting job $1"
duration=$((1+RANDOM%5))
sleep $duration
echo "Job $1 took ${duration} seconds"

[1] 1585
[2] 1586
[3] 1587
[4] 1589
vagrant@tb:~/book/ch08$ Starting job 1
Starting job 2
Starting job 3
Starting job 4
Job 2 took 1 seconds
Processed 2
Job 1 took 2 seconds
Processed 1
Job 3 took 2 seconds
Processed 3
Job 4 took 2 seconds
Processed 4

[1]   Done                    ( ./slow.sh $i; echo Processed $i )
[2]   Done                    ( ./slow.sh $i; echo Processed $i )
[3]-  Done                    ( ./slow.sh $i; echo Processed $i )
[4]+  Done                    ( ./slow.sh $i; echo Processed $i )



for i in {1..4}; do
> (./slow.sh $i; echo Processed $i) &
> done
[1] 1670
[2] 1671
[3] 1673
[4] 1674
vagrant@tb:~/book/ch08$ Starting job 3
Starting job 1
Starting job 4
Starting job 2
Job 3 took 1 seconds
Processed 3
Job 4 took 1 seconds
Processed 4
Job 1 took 2 seconds
Processed 1
Job 2 took 3 seconds
Processed 2

[1]   Done                    ( ./slow.sh $i; echo Processed $i )
[2]   Done                    ( ./slow.sh $i; echo Processed $i )
[3]-  Done                    ( ./slow.sh $i; echo Processed $i )
[4]+  Done                    ( ./slow.sh $i; echo Processed $i )


while read i; do
> (./slow.sh "$i"; ) &
> done < data/movies.txt
[1] 1682
[2] 1683
[3] 1684
[4] 1685
Starting job Matrix
Starting job Home Alone
[5] 1686
vagrant@tb:~/book/ch08$ Starting job Star Wars
Starting job Indiana Jones
Starting job Back to the Future
Job Back to the Future took 2 seconds
Job Matrix took 3 seconds
Job Indiana Jones took 3 seconds
Job Home Alone took 5 seconds
Job Star Wars took 5 seconds

GNU Parallel
seq 5 | parallel "echo {}^2 | bc"
seq 5 | parallel echo {}
< input.csv parallel -C, "mv {1} {2}"
< input.csv parallel -C, --header : "invite {name} {email}"
seq 5 | parallel -N0 "echo The command line rules

**number of job control
seq 5 | parallel -j0 "echo Hi {}"
seq 5 | parallel -j200% "echo Hi {}"

** Logging
seq 5 | parallel "echo \"Hi {}\" > data/hi-{}.txt"
seq 5 | parallel "echo Hi {}" >> data/one-big-file.txt
seq 5 | parallel --results data/outdir "echo Hi {}"

--keep-order

seq 5 | parallel --tag "echo Hi {}"

** pbc
seq 100 | pbc '{1}^2' | tail


** AWS EC2 Instances

====================================================
chapter 9

parallel "curl -sL http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-{}.csv > wine-{}.csv" ::: red white

head -n 5 wine-{red,white}.csv | fold

wc -l wine-{red,white}.csv

• Convert the header to lowercase
• Convert the semicolons to commas
• Convert spaces to underscores
• Remove unnecessary quotes

$ for T in red white; do
> < wine-$T.csv tr '[A-Z]; ' '[a-z],_' | tr -d \" > wine-${T}-clean.csv
> done

** combine two data set
$ HEADER="$(head -n 1 wine-red-clean.csv),type"
$ csvstack -g red,white -n type wine-{red,white}-clean.csv |
> csvcut -c $HEADER > wine-both-clean.csv

**csvstat ... --nulls

$ < wine-both-clean.csv Rio -ge 'g+geom_density(aes(quality, '\
> 'fill=type), adjust=3, alpha=0.5)' | display

$ < wine-both-clean.csv Rio -ge 'ggplot(df, aes(x=alcohol, y=quality, '\
> 'color=type)) + geom_point(position="jitter", alpha=0.2) + '\
> 'geom_smooth(method="lm")' | display

****Dimensionality Reduction with Tapkee
< wine-both.csv cols -C type Rio -f scale > wine-both-scaled.csv


